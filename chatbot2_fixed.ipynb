{"cells":[{"cell_type":"markdown","id":"1a302894","metadata":{"id":"1a302894"},"source":["\n","# Proyecto 5 — Chatbot Conversacional Multimodal para Servicio al Cliente (Banco Digital)\n","\n","**Problema:** Alto volumen de consultas repetitivas (~80%) y necesidad de atención 24/7.  \n","**Solución:** Chatbot inteligente con **NLU (BERT + clasificación de intenciones + extracción de entidades)**, **gestión de diálogo basada en Transformer con memoria**, **pipeline multimodal (texto + OCR de imágenes)**, **API + interfaz web (Gradio)** y **sistema de evaluación**.\n","\n","**Datasets:** [Banking77](https://huggingface.co/datasets/banking77)\n","\n"]},{"cell_type":"markdown","id":"dd810fc1","metadata":{"id":"dd810fc1"},"source":["## 1. Instalación y configuración"]},{"cell_type":"code","execution_count":null,"id":"5fab0ec3","metadata":{"id":"5fab0ec3"},"outputs":[],"source":["\n","# Recomendado ejecutar en Google Colab\n","!pip -q install --upgrade pip\n","!pip -q install transformers datasets accelerate evaluate scikit-learn gradio fastapi uvicorn pyngrok python-multipart pillow pytesseract matplotlib\n","# Opcional: spaCy y modelo liviano en inglés para entidades\n","!pip -q install spacy && python -m spacy download en_core_web_sm\n","# Para mostrar gráficas\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","id":"179067a5","metadata":{"id":"179067a5"},"source":["## 2. Carga del dataset Banking77 (intenciones bancarias)"]},{"cell_type":"code","execution_count":null,"id":"7ebb813b","metadata":{"id":"7ebb813b"},"outputs":[],"source":["\n","from datasets import load_dataset\n","\n","ds = load_dataset(\"banking77\")\n","label_list = ds[\"train\"].features[\"label\"].names\n","num_labels = len(label_list)\n","print(\"Clases:\", num_labels)\n","print(\"Ejemplo:\", ds[\"train\"][0])\n"]},{"cell_type":"markdown","id":"4f14fda7","metadata":{"id":"4f14fda7"},"source":["## 3. Preparación de datos (train/validation/test)"]},{"cell_type":"code","execution_count":null,"id":"828d94fe","metadata":{"id":"828d94fe"},"outputs":[],"source":["\n","from datasets import DatasetDict\n","\n","# Banking77 ya trae split train/test; creamos validación a partir de train\n","train_test_valid = ds\n","train_valid = train_test_valid[\"train\"].train_test_split(test_size=0.1, seed=42)\n","dataset = DatasetDict({\n","    \"train\": train_valid[\"train\"],\n","    \"validation\": train_valid[\"test\"],\n","    \"test\": train_test_valid[\"test\"]\n","})\n","for k in dataset:\n","    print(k, len(dataset[k]))\n"]},{"cell_type":"markdown","id":"cea22bb9","metadata":{"id":"cea22bb9"},"source":["## 4. Baseline: Embeddings básicos (TF‑IDF) + Clasificación Multiclase (LogisticRegression)"]},{"cell_type":"code","execution_count":null,"id":"588d6ccd","metadata":{"id":"588d6ccd"},"outputs":[],"source":["\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n","import pandas as pd\n","\n","# Convertimos a DataFrame para facilidad\n","train_texts = dataset[\"train\"][\"text\"]\n","train_labels = dataset[\"train\"][\"label\"]\n","val_texts = dataset[\"validation\"][\"text\"]\n","val_labels = dataset[\"validation\"][\"label\"]\n","test_texts = dataset[\"test\"][\"text\"]\n","test_labels = dataset[\"test\"][\"label\"]\n","\n","tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=40000)\n","X_train = tfidf.fit_transform(train_texts)\n","X_val = tfidf.transform(val_texts)\n","X_test = tfidf.transform(test_texts)\n","\n","clf = LogisticRegression(max_iter=2000, n_jobs=-1, multi_class=\"auto\")\n","clf.fit(X_train, train_labels)\n","val_pred = clf.predict(X_val)\n","test_pred = clf.predict(X_test)\n","\n","print(\"Baseline (TF‑IDF + LR)\")\n","print(\"Valid Accuracy:\", accuracy_score(val_labels, val_pred))\n","print(\"Valid Macro-F1:\", f1_score(val_labels, val_pred, average=\"macro\"))\n","print(\"Test  Accuracy:\", accuracy_score(test_labels, test_pred))\n","print(\"Test  Macro-F1:\", f1_score(test_labels, test_pred, average=\"macro\"))\n","\n","# Matriz de confusión (conjunto de prueba)\n","cm = confusion_matrix(test_labels, test_pred, labels=list(range(num_labels)))\n","cm_df = pd.DataFrame(cm, index=label_list, columns=label_list)\n","print(\"Matriz de confusión (test) -> usando las 77 etiquetas (se muestra shape):\", cm_df.shape)\n","\n","# Mostrar top‑5 intenciones del baseline (por probabilidad) para una consulta ejemplo\n","def baseline_topk(query, k=5):\n","    probs = clf.predict_proba(tfidf.transform([query]))[0]\n","    idx = np.argsort(probs)[::-1][:k]\n","    return [(label_list[i], float(probs[i])) for i in idx]\n","\n","print(\"Ejemplo baseline_topk('I lost my card'):\", baseline_topk(\"I lost my card\"))\n"]},{"cell_type":"markdown","id":"5cd68983","metadata":{"id":"5cd68983"},"source":["## 5. NLU Avanzado: Fine‑tuning de BERT (transformers Trainer)"]},{"cell_type":"code","execution_count":null,"id":"d22f9c88","metadata":{"id":"d22f9c88"},"outputs":[],"source":["import evaluate\n","from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def tokenize_fn(batch):\n","    return tokenizer(batch[\"text\"], truncation=True)\n","\n","tokenized = dataset.map(tokenize_fn, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","metric_acc = evaluate.load(\"accuracy\")\n","metric_f1 = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    return {\n","        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n","        \"macro_f1\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n","    }\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","args = TrainingArguments(\n","    output_dir=\"bert-banking77\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"macro_f1\",\n","    logging_steps=50,\n","    report_to=\"none\"\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n","print(\"Evaluación (validation):\", trainer.evaluate())\n","\n","# Función de inferencia con BERT\n","import torch\n","id2label = {i:l for i,l in enumerate(label_list)}\n","def bert_infer(text: str, topk:int=5):\n","    enc = tokenizer(text, return_tensors=\"pt\", truncation=True)\n","    with torch.no_grad():\n","        out = model(**enc)\n","        probs = out.logits.softmax(-1)[0].cpu().numpy()\n","    idx = np.argsort(probs)[::-1][:topk]\n","    return [(id2label[i], float(probs[i])) for i in idx]"]},{"cell_type":"markdown","id":"3ebae9ed","metadata":{"id":"3ebae9ed"},"source":["## 6. Extracción de Entidades (ligera)"]},{"cell_type":"code","execution_count":null,"id":"4849a502","metadata":{"id":"4849a502"},"outputs":[],"source":["\n","import re, spacy\n","try:\n","    nlp = spacy.load(\"en_core_web_sm\")\n","except Exception as e:\n","    nlp = None\n","    print(\"spaCy no disponible; se usarán reglas simples:\", e)\n","\n","# Reglas básicas (IBAN, montos, tarjetas, fechas simples)\n","patterns = {\n","    \"iban\": re.compile(r\"\\b[A-Z]{2}\\d{2}[A-Z0-9]{11,30}\\b\"),\n","    \"card\": re.compile(r\"\\b(?:\\d[ -]*?){13,19}\\b\"),\n","    \"amount\": re.compile(r\"\\$?\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\\b\"),\n","}\n","\n","def extract_entities(text: str):\n","    ents = []\n","    # Reglas\n","    for name, pat in patterns.items():\n","        for m in pat.finditer(text):\n","            ents.append({\"label\": name.upper(), \"text\": m.group()})\n","    # spaCy (si está)\n","    if nlp is not None:\n","        doc = nlp(text)\n","        for e in doc.ents:\n","            ents.append({\"label\": e.label_, \"text\": e.text})\n","    return ents\n","\n","print(extract_entities(\"I made a $1,250.00 transfer to IBAN ES91 2100 0418 4502 0005 1332 on 12 May 2024\"))\n"]},{"cell_type":"markdown","id":"b1a3125a","metadata":{"id":"b1a3125a"},"source":["## 7. Gestión de Diálogo (Transformer con memoria corta)"]},{"cell_type":"code","execution_count":null,"id":"2c6cf132","metadata":{"id":"2c6cf132"},"outputs":[],"source":["\n","from transformers import AutoModelForCausalLM, AutoTokenizer as AutoTokCausal\n","import torch\n","\n","dlg_model_name = \"microsoft/DialoGPT-small\"\n","dlg_tok = AutoTokCausal.from_pretrained(dlg_model_name)\n","dlg_model = AutoModelForCausalLM.from_pretrained(dlg_model_name)\n","\n","class MemoryBuffer:\n","    def __init__(self, max_turns=6):\n","        self.buffer = []\n","        self.max_turns = max_turns\n","    def add(self, speaker, text):\n","        self.buffer.append((speaker, text))\n","        self.buffer = self.buffer[-self.max_turns:]\n","    def as_text(self):\n","        return \"\\n\".join([f\"{s}: {t}\" for s,t in self.buffer])\n","\n","memory = MemoryBuffer()\n","\n","def dialog_reply(user_text, max_new_tokens=80):\n","    # Guardamos memoria breve\n","    memory.add(\"User\", user_text)\n","    context = memory.as_text() + \"\\nBot:\"\n","    input_ids = dlg_tok.encode(context, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        out_ids = dlg_model.generate(\n","            input_ids,\n","            max_new_tokens=max_new_tokens,\n","            pad_token_id=dlg_tok.eos_token_id,\n","            do_sample=True,\n","            top_p=0.9,\n","            temperature=0.7\n","        )\n","    reply = dlg_tok.decode(out_ids[0], skip_special_tokens=True).split(\"Bot:\")[-1].strip()\n","    memory.add(\"Bot\", reply)\n","    return reply\n","\n","print(dialog_reply(\"Hello, I lost my card. What should I do?\"))\n"]},{"cell_type":"markdown","id":"d7c5ad17","metadata":{"id":"d7c5ad17"},"source":["## 8. Pipeline Multimodal (OCR con imágenes)"]},{"cell_type":"code","execution_count":null,"id":"d17f706c","metadata":{"id":"d17f706c"},"outputs":[],"source":["\n","import pytesseract\n","from PIL import Image\n","import io\n","\n","def ocr_image(pil_image: Image.Image) -> str:\n","    text = pytesseract.image_to_string(pil_image)\n","    return text\n","\n","# Ejemplo (sube una imagen en Colab y prueba manualmente)\n","# from google.colab import files\n","# up = files.upload()\n","# img = Image.open(next(iter(up)))\n","# print(ocr_image(img))\n"]},{"cell_type":"code","source":["# Actualiza pyngrok\n","!pip install --upgrade pyngrok"],"metadata":{"id":"4XMItJ2Era6b"},"id":"4XMItJ2Era6b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# ⚠️ Usas tu token (el que me diste). Si prefieres, cámbialo antes de ejecutar.\n","os.environ[\"NGROK_AUTH_TOKEN\"] = \"30tccZE47Hfv1TzGnt0B2uqgc6q_2Rr75hXnvLFC7PTPhrsSW\"\n","\n","# Confirmación (no imprime el token)\n","print(\"Token seteado en variable de entorno NGROK_AUTH_TOKEN.\")\n"],"metadata":{"id":"3waB30BrVO1O"},"id":"3waB30BrVO1O","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import os\n","\n","token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n","if not token:\n","    raise RuntimeError(\"Falta NGROK_AUTH_TOKEN. Configúralo antes de continuar.\")\n","\n","ngrok.kill() # Kill any existing ngrok processes\n","ngrok.set_auth_token(token)\n","public_url = ngrok.connect(8080)\n","print(\"🌐 Tu API estará en:\", public_url)  # Copia esta URL y úsalas para pruebas (o abre /docs)"],"metadata":{"id":"3JpuZvGSYQ9m"},"id":"3JpuZvGSYQ9m","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prueba segura de conexión\n","try:\n","    tunnel = ngrok.connect(8000)\n","    print(f\"Conexión exitosa! URL pública: {tunnel.public_url}\")\n","except Exception as e:\n","    print(f\"Error al conectar: {e}\")\n","    print(\"Verifica que el token sea válido en: https://dashboard.ngrok.com/status/tunnels\")"],"metadata":{"id":"xx8EMsw9qPJN"},"id":"xx8EMsw9qPJN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fastapi import FastAPI, UploadFile, File\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel\n","from io import BytesIO\n","from PIL import Image\n","\n","app = FastAPI(title=\"Banking Chatbot API\")\n","\n","# CORS (ajústalo en producción)\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","class Query(BaseModel):\n","    text: str\n","    topk: int = 5\n","\n","@app.post(\"/predict_intent\")\n","def api_predict_intent(q: Query):\n","    # BERT preferente; fallback al baseline si BERT no está disponible\n","    try:\n","        top = bert_infer(q.text, topk=q.topk)\n","    except Exception:\n","        top = baseline_topk(q.text, k=q.topk)\n","    return {\"intents\": [{\"label\": l, \"score\": float(s)} for l, s in top]}\n","\n","@app.post(\"/extract_entities\")\n","def api_extract_entities(q: Query):\n","    ents = extract_entities(q.text)\n","    return {\"entities\": ents}\n","\n","@app.post(\"/chat\")\n","def api_chat(q: Query):\n","    reply = dialog_reply(q.text)\n","    return {\"reply\": reply, \"memory\": memory.as_text()}\n","\n","@app.post(\"/ocr\")\n","async def api_ocr(file: UploadFile = File(...)):\n","    content = await file.read()\n","    img = Image.open(BytesIO(content)).convert(\"RGB\")\n","    text = ocr_image(img)\n","    ents = extract_entities(text)\n","    return {\"text\": text, \"entities\": ents}\n"],"metadata":{"id":"Ye1ukDI6Zy0P"},"id":"Ye1ukDI6Zy0P","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- One-cell launcher: FastAPI + nest_asyncio + ngrok (Colab/Jupyter) ---\n","\n","# (1) Dependencias (si ya las tienes, puedes comentar estas líneas)\n","!pip -q install fastapi uvicorn pyngrok nest_asyncio\n","\n","# (2) Imports\n","import os, threading, time\n","import nest_asyncio\n","from pyngrok import ngrok\n","\n","# (3) Tu app FastAPI debe existir como variable `app`.\n","#     Si ya la definiste arriba, NO ejecutes este ejemplo.\n","try:\n","    app  # noqa: F821\n","except NameError:\n","    # --- EJEMPLO (bórralo si ya tienes `app`) ---\n","    from fastapi import FastAPI\n","    app = FastAPI(title=\"Demo API\")\n","    @app.get(\"/\")\n","    def root():\n","        return {\"status\": \"ok\", \"msg\": \"FastAPI + ngrok listo 🎯\"}\n","    # --- FIN EJEMPLO ---\n","\n","# (4) Token de ngrok (tómalo de variable de entorno)\n","#     Si ya configuraste el token antes: usa ese y no lo escribas en claro aquí.\n","token = os.getenv(\"NGROK_AUTH_TOKEN\", \"\").strip()\n","if not token:\n","    raise RuntimeError(\n","        \"Falta NGROK_AUTH_TOKEN. Configúralo antes, por ejemplo:\\n\"\n","        \"import os; os.environ['NGROK_AUTH_TOKEN'] = 'TU_TOKEN_AQUI'\"\n","    )\n","\n","# (5) Limpiar túneles previos y abrir uno nuevo en 8081\n","ngrok.kill()\n","ngrok.set_auth_token(token)\n","public_url = ngrok.connect(8081)\n","print(\"🌐 URL pública:\", public_url)\n","\n","# (6) Parchear el event loop de Jupyter/Colab y arrancar Uvicorn en un hilo\n","nest_asyncio.apply()\n","\n","def _run_uvicorn():\n","    import uvicorn\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8081, log_level=\"info\")\n","\n","t = threading.Thread(target=_run_uvicorn, daemon=True)\n","t.start()\n","\n","# (7) Esperar un momento y mostrar /docs\n","time.sleep(2)\n","print(\"📚 Swagger UI:\", f\"{public_url}/docs\")\n","\n"],"metadata":{"id":"i6xvi2P9Z3gE"},"id":"i6xvi2P9Z3gE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","\n","BASE = \"https://7cf4835be121.ngrok-free.app\"  # tu URL pública real\n","resp = requests.post(f\"{BASE}/predict_intent\", json={\"text\": \"I lost my card\", \"topk\": 5})\n","print(resp.status_code, resp.json())\n"],"metadata":{"id":"bMx8G8JAlbY-"},"id":"bMx8G8JAlbY-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","def ui_intent(text):\n","    try:\n","        top = bert_infer(text, topk=5)\n","    except Exception:\n","        top = baseline_topk(text, k=5)\n","    return \"\\n\".join([f\"{i+1}. {l}: {s:.3f}\" for i,(l,s) in enumerate(top)])\n","\n","def ui_entities(text):\n","    ents = extract_entities(text)\n","    return \"\\n\".join([f\"{e['label']}: {e['text']}\" for e in ents]) if ents else \"(no entities)\"\n","\n","def ui_chat(text):\n","    return dialog_reply(text)\n","\n","def ui_ocr(img):\n","    if img is None:\n","        return \"(no image)\", \"(no entities)\"\n","    text = ocr_image(img)\n","    ents = extract_entities(text)\n","    ents_str = \"\\n\".join([f\"{e['label']}: {e['text']}\" for e in ents]) if ents else \"(no entities)\"\n","    return text, ents_str\n","\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"# Banking Chatbot (Multimodal)\")\n","    with gr.Tab(\"Intent Classification\"):\n","        inp = gr.Textbox(label=\"Consulta del cliente\")\n","        out = gr.Textbox(label=\"Top-5 intenciones\")\n","        gr.Button(\"Clasificar\").click(ui_intent, inputs=inp, outputs=out)\n","    with gr.Tab(\"Entities\"):\n","        inp2 = gr.Textbox(label=\"Texto\")\n","        out2 = gr.Textbox(label=\"Entidades detectadas\")\n","        gr.Button(\"Extraer\").click(ui_entities, inputs=inp2, outputs=out2)\n","    with gr.Tab(\"Chat\"):\n","        chat_in = gr.Textbox(label=\"Escribe aquí\")\n","        chat_out = gr.Textbox(label=\"Respuesta\")\n","        gr.Button(\"Enviar\").click(ui_chat, inputs=chat_in, outputs=chat_out)\n","    with gr.Tab(\"OCR\"):\n","        img_in = gr.Image(type=\"pil\", label=\"Imagen (voucher/captura)\")\n","        text_out = gr.Textbox(label=\"Texto OCR\")\n","        ents_out = gr.Textbox(label=\"Entidades\")\n","        gr.Button(\"Procesar\").click(ui_ocr, inputs=img_in, outputs=[text_out, ents_out])\n","\n","demo.launch(share=False)\n"],"metadata":{"id":"r7FT2z9thcIG"},"id":"r7FT2z9thcIG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pydantic import BaseModel\n","feedback_log = []\n","\n","class Feedback(BaseModel):\n","    text: str = \"\"\n","    helpful: bool | None = None   # o usa score: int = 1..5\n","    score: int | None = None\n","\n","@app.post(\"/feedback\")\n","def api_feedback(f: Feedback):\n","    feedback_log.append(f.dict())\n","    return {\"ok\": True, \"count\": len(feedback_log)}\n","\n","@app.get(\"/feedback_stats\")\n","def api_feedback_stats():\n","    n = len(feedback_log)\n","    if n == 0:\n","        return {\"n\": 0, \"avg_score\": None, \"helpful_rate\": None}\n","    scores = [x.get(\"score\") for x in feedback_log if x.get(\"score\") is not None]\n","    avg = sum(scores)/len(scores) if scores else None\n","    helpful_flags = [x.get(\"helpful\") for x in feedback_log if x.get(\"helpful\") is not None]\n","    helpful_rate = (sum(1 for h in helpful_flags if h) / len(helpful_flags)) if helpful_flags else None\n","    return {\"n\": n, \"avg_score\": avg, \"helpful_rate\": helpful_rate}\n"],"metadata":{"id":"S-I5yiUJ1ORH"},"id":"S-I5yiUJ1ORH","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3db29951","metadata":{"id":"3db29951"},"source":["## 9. API con FastAPI + ngrok"]},{"cell_type":"code","source":["%cd /content/chatbot_multimodal_project\n","!python run_ngrok.py\n"],"metadata":{"id":"sanQhAeZXOWe"},"id":"sanQhAeZXOWe","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c6f9c0e9","metadata":{"id":"c6f9c0e9"},"source":["## 11. Evaluación y Reportes"]},{"cell_type":"code","execution_count":null,"id":"eca23fe8","metadata":{"id":"eca23fe8"},"outputs":[],"source":["\n","# Evaluación final en test (baseline vs BERT si está)\n","try:\n","    # BERT eval\n","    bert_logits = []\n","    import torch\n","    for b in range(0, len(tokenized[\"test\"]), 64):\n","        batch = tokenized[\"test\"][b:b+64]\n","        enc = tokenizer(batch[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n","        with torch.no_grad():\n","            out = model(**enc).logits\n","        bert_logits.append(out.cpu().numpy())\n","    import numpy as np\n","    bert_logits = np.vstack(bert_logits)\n","    bert_preds = bert_logits.argmax(axis=1)\n","    bert_acc = accuracy_score(dataset[\"test\"][\"label\"], bert_preds)\n","    bert_f1 = f1_score(dataset[\"test\"][\"label\"], bert_preds, average=\"macro\")\n","    print(f\"BERT Test Accuracy: {bert_acc:.4f}  Macro-F1: {bert_f1:.4f}\")\n","except Exception as e:\n","    print(\"BERT evaluación omitida:\", e)\n","\n","# Baseline eval (ya calculado). Reporte de clasificación sobre test:\n","print(\"\\nBaseline (TF‑IDF + LR) - Classification report (test):\")\n","print(classification_report(test_labels, test_pred, target_names=label_list, digits=4))\n","\n","# Gráfico: matriz de confusión (baseline)\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(6,5))\n","plt.imshow(cm, interpolation='nearest')\n","plt.title('Confusion Matrix (Baseline)')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.colorbar()\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","id":"5ab4098b","metadata":{"id":"5ab4098b"},"source":["\n","## 12. Plataformas necesarias y notas de ejecución\n","\n","- **Google Colab**: ejecutar este notebook (GPU opcional para BERT).\n","- **Hugging Face `datasets` y `transformers`** para NLU (Banking77 + BERT).\n","- **scikit‑learn** para baseline TF‑IDF + LogisticRegression.\n","- **FastAPI + Uvicorn + pyngrok** para exponer la **API** y obtener una URL pública temporal.\n","- **Gradio** para la **interfaz web**.\n","- **pytesseract** para OCR (pipeline multimodal).\n","\n","> Para producción, considerar: endpoints en un servidor propio, almacenamiento de logs/feedback en DB, autenticación y monitoreo.\n"]},{"cell_type":"code","metadata":{"id":"84d0910f"},"source":["!pip install pyngrok -q"],"id":"84d0910f","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}